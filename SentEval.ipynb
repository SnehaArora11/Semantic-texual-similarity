{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from urllib.request import urlretrieve\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# Download pre-trained InferSent model\n",
        "url = 'https://dl.fbaipublicfiles.com/infersent/infersent1.pkl'\n",
        "urlretrieve(url, 'infersent1.pkl')\n",
        "\n",
        "# Download GloVe embeddings (this will take some time)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip GloVe embeddings\n",
        "!unzip glove.6B.zip -d glove\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDSSFMoRbW6f",
        "outputId": "63f74cd6-d00b-4993-cc0a-e669269ed291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "--2024-11-11 20:37:55--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-11 20:37:55--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-11 20:37:55--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2024-11-11 20:40:34 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InferSent(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YJ20sslWdRQE",
        "outputId": "16eca1eb-d1cc-439f-97da-439bf3cdc07a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 18:49:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-11 18:49:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-11 18:49:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-11-11 18:51:41 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n",
            "Vocab size : 100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-bd2f035bfa33>:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('/content/InferSent/infersent1.pkl', map_location=device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bd2f035bfa33>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Calculate and print similarity score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Similarity Score (0-5): {similarity}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bd2f035bfa33>\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(sentence1, sentence2)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Encode the sentences using the InferSent model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Compute the cosine similarity between the two sentence embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/InferSent/models.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, bsize, tokenize, verbose)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         sentences, lengths, idx_sort = self.prepare_samples(\n\u001b[0m\u001b[1;32m    214\u001b[0m                         sentences, bsize, tokenize, verbose)\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/InferSent/models.py\u001b[0m in \u001b[0;36mprepare_samples\u001b[0;34m(self, sentences, bsize, tokenize, verbose)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n\u001b[0m\u001b[1;32m    186\u001b[0m                      [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n\u001b[1;32m    187\u001b[0m         \u001b[0mn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/InferSent/models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n\u001b[0;32m--> 186\u001b[0;31m                      [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/InferSent/models.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_infersent_model():\n",
        "    params_model = {\n",
        "        'word_emb_dim': 300,\n",
        "        'bsize': 64,\n",
        "        'enc_lstm_dim': 2048,\n",
        "        'pool_type': 'mean',\n",
        "        'dpout_model': 0.0,\n",
        "        'version': 1,\n",
        "    }\n",
        "    infer_sent = InferSent(params_model)\n",
        "\n",
        "    # Load GloVe embeddings\n",
        "    glove_path = '/content/InferSent/glove/glove.6B.300d.txt'\n",
        "    infer_sent.set_w2v_path(glove_path)\n",
        "    infer_sent.build_vocab_k_words(K=100000)  # You can change K to fit your needs\n",
        "    return infer_sent"
      ],
      "metadata": {
        "id": "hRMRC56dnVH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def calculate_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Given two sentences, calculate the similarity score between 0-5 using InferSent.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentences\n",
        "    sentences = [sentence1, sentence2]\n",
        "\n",
        "    # Encode the sentences using the InferSent model\n",
        "    embeddings = infer_sent.encode(sentences, tokenize=False)\n",
        "\n",
        "    # Compute the cosine similarity between the two sentence embeddings\n",
        "    cos_sim = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
        "\n",
        "    # Normalize the similarity score to be between 0 and 5\n",
        "    similarity = (cos_sim + 1) * 2.5  # Cosine similarity is between -1 and 1, so we scale it to be between 0 and 5\n",
        "\n",
        "    return similarity\n",
        "# sentence1 = \"I love machine learning.\"\n",
        "# sentence2 = \"I love machine learning\"\n",
        "# sentence1 = \"The cat sat on the cozy sofa by the window.\"\n",
        "# sentence2 = \"A cat was relaxing on a comfortable couch near the window.\"\n",
        "# sentence1 = \"The quick brown fox jumped over the lazy dog.\"\n",
        "# sentence2 = \"A last emrald of the pearl city green bird barks over a active cat.\"\n",
        "# sentence1 = \"The sun sets over the ocean.\"\n",
        "# sentence2 = \"The car engine makes a loud noise.\"\n",
        "# sentence1 = \"I hate machine learning.\"\n",
        "# sentence2 = \"Cricket is specifically bad.\"\n",
        "\n",
        "# Calculate and print similarity score\n",
        "sentence1 = input(\"Enter Sentence1: \") #\"I hate machine learning.\"\n",
        "sentence2 = input(\"Enter Sentence2: \") # \"Cricket is specifically bad.\"\n",
        "similarity = calculate_similarity(sentence1, sentence2)\n",
        "# print(f\"Similarity Score (0-5): {similarity}\")\n",
        "\n",
        "print(f\"Cosine Similarity (0-5 scale): {similarity}\")\n",
        "print(\"--\"*30)\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
        "print(\"--\"*30)\n",
        "print(f\"Spearman Rank Correlation: {spearman_corr}\")\n",
        "\n",
        "# This and the above cell works fiine for similar sentence"
      ],
      "metadata": {
        "id": "RMiJGd2CkiOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from models import InferSent\n",
        "import nltk\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ensure punkt tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the InferSent model\n",
        "params_model = {\n",
        "    'word_emb_dim': 300,\n",
        "    'bsize': 64,\n",
        "    'enc_lstm_dim': 2048,\n",
        "    'pool_type': 'max',\n",
        "    'dpout_model': 0.0,\n",
        "    'version': 1,\n",
        "}\n",
        "infer_sent = InferSent(params_model)\n",
        "\n",
        "# Set the GloVe word vectors path\n",
        "\n",
        "\n",
        "# Build vocabulary with the most frequent words (adjust K based on your dataset)\n",
        "# infer_sent.build_vocab_k_words(K=100000)\n",
        "\n",
        "# Load the pre-trained weights\n",
        "model_path = '/content/InferSent/infersent1.pkl'\n",
        "infer_sent.load_state_dict(torch.load(model_path))\n",
        "glove_path = '/content/InferSent/glove/glove.6B.300d.txt'\n",
        "infer_sent.set_w2v_path(glove_path)\n",
        "sentence1 = input(\"Enter Sentence1: \") #\"I hate machine learning.\"\n",
        "sentence2 = input(\"Enter Sentence2: \") # \"Cricket is specifically bad.\"\n",
        "sentences = [sentence1, sentence2]\n",
        "infer_sent.build_vocab(sentences, tokenize=False)  # Set tokenize=True if you need tokenization\n",
        "\n",
        "\n",
        "# Encode sentences to get embeddings\n",
        "embeddings = infer_sent.encode(sentences, tokenize=False, bsize=64)\n",
        "\n",
        "# Compute cosine similarity between the embeddings\n",
        "similarity = cosine_similarity([embeddings[0]], [embeddings[1]])\n",
        "\n",
        "similarity_score_0_to_5 = (similarity[0][0] + 1) * 2.5  # Scale to 0-5 range\n",
        "\n",
        "pearson_corr, _ = pearsonr(embeddings[0], embeddings[1])\n",
        "\n",
        "# Compute Spearman rank correlation\n",
        "spearman_corr, _ = spearmanr(embeddings[0], embeddings[1])\n",
        "\n",
        "# Print results\n",
        "print(f\"Cosine Similarity (0-5 scale): {similarity_score_0_to_5}\")\n",
        "print(\"--\"*30)\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
        "print(\"--\"*30)\n",
        "print(f\"Spearman Rank Correlation: {spearman_corr}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i0_jiiN1idU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(sentence1, sentence2):\n",
        "    # Ensure sentences are passed as a list of strings (not tokenized)\n",
        "    sentences = [sentence1, sentence2]\n",
        "\n",
        "    # Encode the sentences using the InferSent model\n",
        "    print(f\"Encoding sentences...\")\n",
        "    embeddings = infer_sent.encode(sentences, tokenize=True)\n",
        "\n",
        "    # Compute cosine similarity between the two embeddings\n",
        "    print(f\"Computing similarity...\")\n",
        "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])\n",
        "\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Example sentences\n",
        "sentence1 = \"I love machine learning.\"\n",
        "sentence2 = \"Cricket is fascinating.\"\n",
        "\n",
        "# Calculate and print similarity score\n",
        "similarity = calculate_similarity(sentence1, sentence2)\n",
        "print(f\"Similarity Score (0-5): {similarity}\")"
      ],
      "metadata": {
        "id": "3RxzVTyntnoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}