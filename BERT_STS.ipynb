{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8324479,"sourceType":"datasetVersion","datasetId":4944781},{"sourceId":8356210,"sourceType":"datasetVersion","datasetId":4965424}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer\n#from clinical_bert_similarity import ClinicalBertSimilarity\n\n# Load the saved model\nmodel_path = '/kaggle/input/sick-dataset-model/sick_model.pth'  # Update with the path where you saved the model\nloaded_model = ClinicalBertSimilarity(device='cuda')  # Assuming you want to load the model on GPU\nloaded_model.regressor_net.load_state_dict(torch.load(model_path))\n#loaded_model.to('cuda')  # Move the model to GPU if available\n\n# Define the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Input sentences\ns1 = input(\"Enter the first sentence: \").lower()\ns2 = input(\"Enter the second sentence: \").lower()\n\n# Predict similarity score\npredictions = loaded_model.predict([(s1, s2)])\nprint(\"Similarity score is: \", predictions[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T12:58:06.458461Z","iopub.execute_input":"2024-05-08T12:58:06.458808Z","iopub.status.idle":"2024-05-08T12:58:12.744360Z","shell.execute_reply.started":"2024-05-08T12:58:06.458781Z","shell.execute_reply":"2024-05-08T12:58:12.742914Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#from clinical_bert_similarity import ClinicalBertSimilarity\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/sick-dataset-model/sick_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with the path where you saved the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mClinicalBertSimilarity\u001b[49m(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assuming you want to load the model on GPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mregressor_net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#loaded_model.to('cuda')  # Move the model to GPU if available\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define the tokenizer\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'ClinicalBertSimilarity' is not defined"],"ename":"NameError","evalue":"name 'ClinicalBertSimilarity' is not defined","output_type":"error"}]},{"cell_type":"code","source":"!pip install pytorch-transformers\n!pip install wandb\n!pip install torch torchvision torchaudio\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T12:58:52.613536Z","iopub.execute_input":"2024-05-08T12:58:52.614472Z","iopub.status.idle":"2024-05-08T12:59:36.075087Z","shell.execute_reply.started":"2024-05-08T12:58:52.614426Z","shell.execute_reply":"2024-05-08T12:59:36.073949Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pytorch-transformers\n  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (1.26.4)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (4.66.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (2023.12.25)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from pytorch-transformers) (0.2.0)\nCollecting sacremoses (from pytorch-transformers)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch-transformers) (2024.2.0)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->pytorch-transformers)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-transformers) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-transformers) (0.6.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-transformers) (2024.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch-transformers) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch-transformers) (1.4.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch-transformers) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch-transformers) (1.16.0)\nDownloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses, botocore, pytorch-transformers\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.69\n    Uninstalling botocore-1.34.69:\n      Successfully uninstalled botocore-1.34.69\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165 pytorch-transformers-1.2.0 sacremoses-0.1.1\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom pytorch_transformers import BertTokenizer, BertConfig, BertModel\nfrom pytorch_transformers.modeling_utils import CONFIG_NAME\nfrom torch import nn\nfrom scipy.stats import spearmanr\nfrom pytorch_transformers.file_utils import url_to_filename, http_get\nimport tarfile\nimport tempfile\nimport torch.optim as optim\nimport torch \nimport os\nfrom scipy.stats import pearsonr, spearmanr\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_transformers import BertTokenizer\nfrom pytorch_transformers.modeling_utils import WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_transformers.modeling_bert import BertPreTrainedModel, BertConfig, BertModel\nimport os, tarfile, urllib, tempfile, shutil\n#from pytorch_transformers.file_utils import url_to_filename, http_get\nimport wandb\nwandb.init(project='Semantic_textual_similarity', entity='bluehoax')\n\n\ntry:\n    from torch.hub import _get_torch_home\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv('TORCH_HOME', os.path.join(\n            os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n\ndefault_cache_path = os.path.join(torch_cache_home, 'semantic_text_similarity')\n\nMODEL_URL = {\n    'clinical-bert-similarity': 'https://github.com/AndriyMulyar/semantic-text-similarity/releases/download/v1.0.0/clinical_bert_similarity.tar.gz',\n    'web-bert-similarity': 'https://github.com/AndriyMulyar/semantic-text-similarity/releases/download/v1.0.0/web_bert_similarity.tar.gz'\n}\ndef get_model_path(model_name: str):\n\n    if model_name in MODEL_URL:\n        model_url = MODEL_URL[model_name]\n    else:\n        return model_name\n    model_url_hash = url_to_filename(model_url)\n\n    model_path = os.path.join(default_cache_path, model_url_hash)\n\n    if os.path.exists(model_path):\n        return model_path\n    else:\n        os.makedirs(model_path)\n        with tempfile.NamedTemporaryFile() as temp_file:\n            print(\"Downloading model: %s from %s\" % (model_name, model_url))\n            try:\n                http_get(model_url, temp_file)\n                tar = tarfile.open(temp_file.name)\n            except BaseException as exc:\n                print(\"Failed to download model: %s\"% model_name)\n                os.rmdir(model_path)\n                raise exc\n\n\n            temp_file.flush()\n            temp_file.seek(0)\n            tar.extractall(model_path)\n\n            return model_path\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length: int):\n    \"\"\"\n    Copied exactly from: https://github.com/huggingface/pytorch-pretrained-BERT/blob/78462aad6113d50063d8251e27dbaadb7f44fbf0/examples/extract_features.py#L150\n    Truncates a sequence pair in place to the maximum length.\n    \"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef bert_sentence_pair_preprocessing(data: list, tokenizer: BertTokenizer, max_sequence_length=128):\n    \"\"\"\n    Pre-processes an array of sentence pairs for input into bert. Sentence pairs are expected to be processed\n    as given in data.py.\n\n    Each sentence pair is tokenized and concatenated together by the [SEP] token for input into BERT\n\n    :return: three tensors: [data_size, input_ids], [data_size, token_type_ids], [data_size, attention_mask]\n    \"\"\"\n\n    max_bert_input_length = 0\n    for sentence_pair in data:\n\n        sentence_1_tokenized, sentence_2_tokenized = tokenizer.tokenize(sentence_pair['sentence_1']), tokenizer.tokenize(sentence_pair['sentence_2'])\n        _truncate_seq_pair(sentence_1_tokenized, sentence_2_tokenized, max_sequence_length - 3) #accounting for positioning tokens\n\n\n        max_bert_input_length = max(max_bert_input_length, len(sentence_1_tokenized) + len(sentence_2_tokenized) + 3)\n        sentence_pair['sentence_1_tokenized'] = sentence_1_tokenized\n        sentence_pair['sentence_2_tokenized'] = sentence_2_tokenized\n\n    dataset_input_ids = torch.empty((len(data), max_bert_input_length), dtype=torch.long)\n    dataset_token_type_ids = torch.empty((len(data), max_bert_input_length), dtype=torch.long)\n    dataset_attention_masks = torch.empty((len(data), max_bert_input_length), dtype=torch.long)\n    dataset_scores = torch.empty((len(data), 1), dtype=torch.float)\n\n    for idx, sentence_pair in enumerate(data):\n        tokens = []\n        input_type_ids = []\n\n        tokens.append(\"[CLS]\")\n        input_type_ids.append(0)\n        for token in sentence_pair['sentence_1_tokenized']:\n            tokens.append(token)\n            input_type_ids.append(0)\n        tokens.append(\"[SEP]\")\n        input_type_ids.append(0)\n\n        for token in sentence_pair['sentence_2_tokenized']:\n            tokens.append(token)\n            input_type_ids.append(1)\n        tokens.append(\"[SEP]\")\n        input_type_ids.append(1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        attention_masks = [1] * len(input_ids)\n        while len(input_ids) < max_bert_input_length:\n            input_ids.append(0)\n            attention_masks.append(0)\n            input_type_ids.append(0)\n\n\n        dataset_input_ids[idx] = torch.tensor(input_ids, dtype=torch.long)\n        dataset_token_type_ids[idx] = torch.tensor(input_type_ids, dtype=torch.long)\n        dataset_attention_masks[idx] = torch.tensor(attention_masks, dtype=torch.long)\n        if 'similarity' not in sentence_pair or sentence_pair['similarity'] is None:\n            dataset_scores[idx] = torch.tensor(float('nan'), dtype=torch.float)\n        else:\n            dataset_scores[idx] = torch.tensor(sentence_pair['similarity'], dtype=torch.float)\n\n\n    return dataset_input_ids, dataset_token_type_ids, dataset_attention_masks, dataset_scores\nclass BertSimilarityRegressor(BertPreTrainedModel):\n    def __init__(self, bert_model_config: BertConfig):\n        super(BertSimilarityRegressor, self).__init__(bert_model_config)\n        self.bert = BertModel(bert_model_config)\n        linear_size = bert_model_config.hidden_size\n        if bert_model_config.pretrained_config_archive_map['additional_features'] is not None:\n            linear_size += bert_model_config.pretrained_config_archive_map['additional_features']\n\n        self.regression = nn.Sequential(\n            nn.Dropout(p=bert_model_config.hidden_dropout_prob),\n            nn.Linear(linear_size, 1)\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        # Initialize weights for linear layer\n        self.regression.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(self, input_ids, token_type_ids, attention_masks, additional_features=None):\n        \"\"\"\n        Feed forward network with one hidden layer.\n        :param input_ids:\n        :param token_type_ids:\n        :param attention_masks:\n        :return:\n        \"\"\"\n        \n        _, pooled_output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n\n        if additional_features is not None:\n            pooled_output = torch.cat((pooled_output, additional_features), dim=1)\n\n        return self.regression(pooled_output)\n\n\nclass BertSimilarity():\n    \"\"\"\n    A class implementing the training and evaluation of a fine-tuned BERT model for sentence pair similarity.\n    \"\"\"\n\n    def __init__(self, args=None, device='cuda', bert_model_path='bert-base-uncased', batch_size=10, learning_rate=5e-5, weight_decay=0, additional_features=None):\n        if args is not None:\n            self.args = vars(args)\n\n        assert device in ['cuda', 'cpu']\n\n        if not args:\n            self.args = {}\n            self.args['bert_model_path'] = bert_model_path\n            self.args['device'] = device\n            self.args['learning_rate'] = learning_rate\n            self.args['weight_decay'] = weight_decay\n            self.args['batch_size'] = batch_size\n\n        self.log = logging.getLogger()\n\n        self.bert_tokenizer = BertTokenizer.from_pretrained(self.args['bert_model_path'])\n        if os.path.exists(self.args['bert_model_path']):\n            if os.path.exists(os.path.join(self.args['bert_model_path'], CONFIG_NAME)):\n                config = BertConfig.from_json_file(os.path.join(self.args['bert_model_path'], CONFIG_NAME))\n            elif os.path.exists(os.path.join(self.args['bert_model_path'], 'bert_config.json')):\n                config = BertConfig.from_json_file(os.path.join(self.args['bert_model_path'], 'bert_config.json'))\n            else:\n                raise ValueError(\"Cannot find a configuration for the BERT model you are attempting to load.\")\n\n        self.loss_function = torch.nn.MSELoss()\n\n        config.pretrained_config_archive_map['additional_features'] = additional_features\n\n        self.regressor_net = BertSimilarityRegressor.from_pretrained(self.args['bert_model_path'], config=config)\n        self.optimizer = torch.optim.Adam(\n            self.regressor_net.parameters(),\n            weight_decay=self.args['weight_decay'],\n            lr=self.args['learning_rate']\n        )\n        self.log.info('Initialized BertSentencePairSimilarity model from %s' % self.args['bert_model_path'])\n\n    def predict(self, data: list, additional_features=None, return_predictions=True):\n        \"\"\"\n        Given a list of sentence pair instances, makes predictions\n        :param data:\n        :param return predictions or log to model directory\n        :return:\n        \"\"\"\n\n        self.regressor_net.to(device=self.args['device'])\n        self.regressor_net.eval()\n\n        with torch.no_grad():\n\n            if data is not None and isinstance(data, list):\n                if isinstance(data[0], dict):\n                    input_ids_eval, token_type_ids_eval, attention_masks_eval, correct_scores_eval = bert_sentence_pair_preprocessing(\n                        data, self.bert_tokenizer)\n                elif isinstance(data[0], tuple):\n                    input_ids_eval, token_type_ids_eval, attention_masks_eval, correct_scores_eval = bert_sentence_pair_preprocessing(\n                        [{'sentence_1': s1, 'sentence_2': s2} for s1, s2 in data], self.bert_tokenizer)\n                else:\n                    raise ValueError(\"Data must be a list of sentence pair tuples\")\n\n            predictions = torch.empty_like(correct_scores_eval)\n            for i in range(0, input_ids_eval.shape[0], self.args['batch_size']):\n                input_id_eval = input_ids_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n                token_type_id_eval = token_type_ids_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n                attention_mask_eval = attention_masks_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n\n                if additional_features is not None:\n                    additional_feature = additional_features[i:i + self.args['batch_size']].to(device=self.args['device'])\n                else:\n                    additional_feature = None\n\n                predicted_score = self.regressor_net(input_id_eval, token_type_id_eval, attention_mask_eval, additional_features=additional_feature)\n                predictions[i:i + self.args['batch_size']] = predicted_score\n\n            if all(torch.isnan(correct_scores_eval)) or return_predictions:  # No correct score labels are present, return the predictions\n                return predictions.cpu().view(-1).numpy()\n            else:\n                self.log.info('Evaluating on Epoch %i' % (self.epoch))\n\n                # Calculate Spearman correlation coefficient\n                spearman_corr, _ = spearmanr(predictions.cpu().view(-1).numpy(), correct_scores_eval.view(-1).numpy())\n                self.log.info(\"Spearman: %f\" % spearman_corr)\n\n                with open(os.path.join(self.args['model_directory'], \"eval_%s.csv\" % self.epoch), 'w') as eval_results:\n                    eval_results.write(\"sentence_1\\tsentence_2\\tannotator_score\\tpredicated_score\\n\")\n\n                    for idx, row in enumerate(torch.cat((correct_scores_eval, predictions.cpu()), dim=1)):\n                        eval_results.write(\n                            \"%s\\t%s\\t%f\\t%f\\n\" % (data[idx]['sentence_1'], data[idx]['sentence_2'], row[0].item(), row[1].item()))\n                    eval_results.write(\"Spearman\\t%f\" % spearman_corr)\n\n                self.log.info(\"Spearman: %f\" % spearman_corr)\n        self.regressor_net.train()\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length: int):\n    \"\"\"\n    Copied exactly from: https://github.com/huggingface/pytorch-pretrained-BERT/blob/78462aad6113d50063d8251e27dbaadb7f44fbf0/examples/extract_features.py#L150\n    Truncates a sequence pair in place to the maximum length.\n    \"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\nclass STSBDataset(Dataset):\n    def __init__(self, data, tokenizer, max_sequence_length=64):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_sequence_length = max_sequence_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        input_ids, token_type_ids, attention_masks, labels = self.tokenize_and_convert_to_tensor(sample['sentence_1'], sample['sentence_2'], sample['similarity'])\n        \n        # Convert lists to tensors\n        input_ids = torch.tensor(input_ids)\n        token_type_ids = torch.tensor(token_type_ids)\n        attention_masks = torch.tensor(attention_masks)\n        labels = torch.tensor(labels)\n        \n        return input_ids, token_type_ids, attention_masks, labels\n\n    def tokenize_and_convert_to_tensor(self, sentence_1, sentence_2, similarity_score):\n        sentence_1_tokenized = self.tokenizer.tokenize(sentence_1)\n        sentence_2_tokenized = self.tokenizer.tokenize(sentence_2)\n\n        # Truncate sequences if they exceed max_sequence_length - 3 for [CLS], [SEP], [SEP]\n        _truncate_seq_pair(sentence_1_tokenized, sentence_2_tokenized, self.max_sequence_length - 3)\n\n        # Add [CLS] and [SEP] tokens, and generate input_ids, token_type_ids, and attention_masks\n        input_ids = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence_1_tokenized + ['[SEP]'] + sentence_2_tokenized + ['[SEP]'])\n        token_type_ids = [0] * (len(sentence_1_tokenized) + 2) + [1] * (len(sentence_2_tokenized) + 1)\n        attention_masks = [1] * len(input_ids)\n\n        # Pad sequences to max_sequence_length\n        input_ids += [0] * (self.max_sequence_length - len(input_ids))\n        token_type_ids += [0] * (self.max_sequence_length - len(token_type_ids))\n        attention_masks += [0] * (self.max_sequence_length - len(attention_masks))\n\n        labels = similarity_score  # Assuming similarity_score is a float\n        return input_ids, token_type_ids, attention_masks, labels\n\ndef load_sts_b_data(tokenizer):\n    sick_file_path = '/kaggle/input/sickdataset/SICK.txt'\n    if not os.path.exists(sick_file_path):\n        raise FileNotFoundError('Cannot find SICK dataset')\n\n    with open(sick_file_path, 'r', encoding='utf-8') as file:\n        file.readline()  # Skip header line\n        sick_data = file.readlines()\n\n    total_samples = len(sick_data)\n    train_size = int(0.8 * total_samples)\n    dev_size = int(0.1 * total_samples)\n\n    train_data = sick_data[:train_size]\n    dev_data = sick_data[train_size:train_size + dev_size]\n    test_data = sick_data[train_size + dev_size:]\n\n    def process_data(data):\n        processed_data = []\n        for line in data:\n            parts = line.strip().split('\\t')\n            pair_ID, sentence_A, sentence_B, _, relatedness_score, _, _, _, _, _, l, k = parts\n\n            processed_data.append({\n                'sentence_1': sentence_A,\n                'sentence_2': sentence_B,\n                'similarity': float(relatedness_score)\n            })\n        return processed_data\n\n    train_data = process_data(train_data)\n    dev_data = process_data(dev_data)\n    test_data = process_data(test_data)\n\n    return train_data, dev_data, test_data\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Load the data\ntrain_data, dev_data, test_data = load_sts_b_data(tokenizer)\n\n# Create DataLoader instances for train, dev, and test sets\nbatch_size = 10\ntrain_loader = DataLoader(STSBDataset(train_data, tokenizer), batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(STSBDataset(test_data,tokenizer),batch_size=batch_size)\nclass ClinicalBertSimilarity(BertSimilarity):\n    def __init__(self, device='cuda', batch_size=10, model_name=\"web-bert-similarity\"):\n        model_path = get_model_path(model_name)\n        super().__init__(device=device, batch_size=batch_size, bert_model_path=model_path)\n        self.device = torch.device(device)  # Convert device to a Torch device object\n        self.regressor_net.to(self.device)   # Move model to CUDA device\n\n    def train_model(self, train_loader, num_epochs=10, validation_loader=None):\n        # Set the model to training mode\n        self.regressor_net.train()\n\n        # Define loss function and optimizer\n        criterion = torch.nn.MSELoss()\n        optimizer = torch.optim.Adam(self.regressor_net.parameters(), lr=self.args['learning_rate'], weight_decay=self.args['weight_decay'])\n\n        for epoch in range(num_epochs):\n            total_loss = 0.0\n            tqdm_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n            for i, batch in enumerate(tqdm_bar):\n                input_ids, token_type_ids, attention_masks, labels = batch\n                # Move data to device (CUDA)\n                input_ids, token_type_ids, attention_masks, labels = input_ids.to(self.device), token_type_ids.to(self.device), attention_masks.to(self.device), labels.to(self.device)\n\n                # Zero the gradients\n                optimizer.zero_grad() \n                outputs = self.regressor_net(input_ids, token_type_ids, attention_masks)\n\n                labels = labels.unsqueeze(1)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            avg_loss = total_loss / len(train_loader)\n            wandb.log({\"Training Loss\": avg_loss})\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}\")\n\n    \n    def predict(self, data: list, actual_scores=None, additional_features=None, return_predictions=True):\n        \"\"\"\n        Given a list of sentence pair instances, makes predictions\n        :param data: List of sentence pairs\n        :param actual_scores: Actual similarity scores corresponding to the data (optional)\n        :param additional_features: Additional features to include in prediction (optional)\n        :param return_predictions: Whether to return predictions or log to model directory\n        :return: Predictions or Spearman correlation coefficient (if actual_scores provided)\n        \"\"\"\n\n        self.regressor_net.to(device=self.args['device'])\n        self.regressor_net.eval()\n\n        with torch.no_grad():\n            if data is not None and isinstance(data, list):\n                if isinstance(data[0], dict):\n                    input_ids_eval, token_type_ids_eval, attention_masks_eval, correct_scores_eval = bert_sentence_pair_preprocessing(\n                        data, self.bert_tokenizer)\n                elif isinstance(data[0], tuple):\n                    input_ids_eval, token_type_ids_eval, attention_masks_eval, correct_scores_eval = bert_sentence_pair_preprocessing(\n                        [{'sentence_1': s1, 'sentence_2': s2} for s1, s2 in data], self.bert_tokenizer)\n                else:\n                    raise ValueError(\"Data must be a list of sentence pair tuples\")\n\n            predictions = torch.empty_like(correct_scores_eval)\n            for i in range(0, input_ids_eval.shape[0], self.args['batch_size']):\n                input_id_eval = input_ids_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n                token_type_id_eval = token_type_ids_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n                attention_mask_eval = attention_masks_eval[i:i + self.args['batch_size']].to(device=self.args['device'])\n\n                if additional_features is not None:\n                    additional_feature = additional_features[i:i + self.args['batch_size']].to(device=self.args['device'])\n                else:\n                    additional_feature = None\n\n                predicted_score = self.regressor_net(input_id_eval, token_type_id_eval, attention_mask_eval, additional_features=additional_feature)\n                predictions[i:i + self.args['batch_size']] = predicted_score\n\n            if all(torch.isnan(correct_scores_eval)) or return_predictions:  \n                # No correct score labels are present or return predictions is requested, return the predictions\n                return predictions.cpu().view(-1).numpy()\n#             else:\n#                 # Calculate Spearman correlation coefficient if actual scores are provided\n#                 spearman_corr, _ = spearmanr(predictions.cpu().view(-1).numpy(), actual_scores.view(-1).numpy())\n#                 return spearman_corr\n\n    def save_model(self, filename):\n        torch.save(self.regressor_net.state_dict(), filename)\ndevice=\"cuda\"\nbatch_size=10\nmodel = ClinicalBertSimilarity(device, batch_size)\n\n# Train the model\nmodel.train_model(train_loader,5)\n\n\n# Use the predict function to get predictions\npredictions = model.predict(test_data)\npredictions_dev=model.predict(dev_data)\n# Extract the actual similarity scores from the test data\nactual_scores = [sample['similarity'] for sample in test_data]\nactual_scores_dev=[sample['similarity'] for sample in dev_data]\n# Calculate Spearman correlation\nspearman_corr, _ = spearmanr(predictions, actual_scores)\nprint(\"Spearman correlation on test:\", spearman_corr)\n\n# Optionally, you can also calculate Pearson correlation\npearson_corr, _ = pearsonr(predictions, actual_scores)\nprint(\"Pearson correlation on test:\", pearson_corr)\n\nspearman_corr, _ = spearmanr(predictions_dev, actual_scores_dev)\npearson_corr, _ = pearsonr(predictions_dev, actual_scores_dev)\nprint(\"spearman correlation on dev:\",spearman_corr)\nprint(\"pearson correlation on dev:\",pearson_corr)\n# Save the trained model\nmodel.save_model('sick_model.pth')\nprint(\"Model saved successfully\")\n\n\n    \n\ns1 = input(\"Enter the first sentence: \").lower()\ns2 = input(\"Enter the second sentence: \").lower()\npredictions = model.predict([(s1, s2)])\nprint(\"Similarity score is: \", predictions[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T12:59:51.148266Z","iopub.execute_input":"2024-05-08T12:59:51.148933Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240508_130014-y3jrktj7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bluehoax/Semantic_textual_similarity/runs/y3jrktj7' target=\"_blank\">noble-durian-3</a></strong> to <a href='https://wandb.ai/bluehoax/Semantic_textual_similarity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bluehoax/Semantic_textual_similarity' target=\"_blank\">https://wandb.ai/bluehoax/Semantic_textual_similarity</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bluehoax/Semantic_textual_similarity/runs/y3jrktj7' target=\"_blank\">https://wandb.ai/bluehoax/Semantic_textual_similarity/runs/y3jrktj7</a>"},"metadata":{}},{"name":"stderr","text":"100%|██████████| 231508/231508 [00:00<00:00, 4148271.43B/s]\n","output_type":"stream"},{"name":"stdout","text":"Downloading model: web-bert-similarity from https://github.com/AndriyMulyar/semantic-text-similarity/releases/download/v1.0.0/web_bert_similarity.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 405359924/405359924 [00:07<00:00, 52474825.06B/s]\nEpoch 1/10: 100%|██████████| 788/788 [01:17<00:00, 10.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 0.3012608275411953\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 0.19917849675860955\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 0.1492584866417222\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 0.13069379004191248\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 0.11445510190637395\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 0.09850473104749197\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 788/788 [01:16<00:00, 10.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 0.09705082197593251\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 788/788 [01:16<00:00, 10.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 0.08630332436299112\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 0.07313234795612959\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 788/788 [01:15<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 0.07306513479416764\nSpearman correlation on test: 0.9017261361350308\nPearson correlation on test: 0.9254133815962392\nspearman correlation on dev: 0.7541430467708301\npearson correlation on dev: 0.7736874106319803\nModel saved successfully\n","output_type":"stream"}]}]}